From 8df81d9f1d490178449881bfeb06877cd50aacdb Mon Sep 17 00:00:00 2001
From: javachen <june.chan@foxmail.com>
Date: Thu, 26 Dec 2013 13:32:33 +0800
Subject: [PATCH 12/19] add-hbase.hregion.majorcompaction.cron

---
 pom.xml                                            |   6 +
 .../apache/hadoop/hbase/regionserver/Store.java    | 190 +++++++++++++--------
 2 files changed, 128 insertions(+), 68 deletions(-)

diff --git a/pom.xml b/pom.xml
index 65081c6..3a7e950 100644
--- a/pom.xml
+++ b/pom.xml
@@ -1144,6 +1144,12 @@
 		<version>1.7.4</version>
 	</dependency>
 
+	<dependency>
+		<groupId>quartz</groupId>
+		<artifactId>quartz</artifactId>
+		<version>1.5.2</version>
+	</dependency>
+
     <!-- General dependencies -->
     <dependency>
       <groupId>com.yammer.metrics</groupId>
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java b/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
index b1eb55b..54a8d51 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
@@ -59,7 +59,6 @@ import org.apache.hadoop.hbase.blobStore.BlobStore;
 import org.apache.hadoop.hbase.blobStore.BlobStoreManager;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.fs.HFileSystem;
-import org.apache.hadoop.hbase.io.HFileLink;
 import org.apache.hadoop.hbase.io.HeapSize;
 import org.apache.hadoop.hbase.io.hfile.CacheConfig;
 import org.apache.hadoop.hbase.io.hfile.Compression;
@@ -81,8 +80,10 @@ import org.apache.hadoop.hbase.util.ClassSize;
 import org.apache.hadoop.hbase.util.CollectionBackedScanner;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 import org.apache.hadoop.hbase.util.FSUtils;
+import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.ThreadSafeSimpleDateFormat;
 import org.apache.hadoop.util.StringUtils;
+import org.quartz.CronTrigger;
 
 import com.google.common.base.Preconditions;
 import com.google.common.base.Predicate;
@@ -170,11 +171,15 @@ public class Store extends SchemaConfigured implements HeapSize {
 
   // Comparing KeyValues
   final KeyValue.KVComparator comparator;
+  
+  private CronTrigger majorcompactionCronTrigger;
+
 
   private final Compactor compactor;
 
   private final Compression.Algorithm compression;
 
+
   private static final ThreadSafeSimpleDateFormat dateFormatter = new ThreadSafeSimpleDateFormat(
       "yyyyMMdd");;
   /**
@@ -249,7 +254,19 @@ public class Store extends SchemaConfigured implements HeapSize {
       Store.closeCheckInterval = conf.getInt(
           "hbase.hstore.close.check.interval", 10*1000*1000 /* 10 MB */);
     }
-    this.storefiles = sortAndClone(loadStoreFiles());
+    String majorCompactionCron = this.conf.get("hbase.hregion.majorcompaction.cron");
+    if ((majorCompactionCron != null) && (!majorCompactionCron.isEmpty())) {
+      try {
+        this.majorcompactionCronTrigger = new CronTrigger(toString(), "MajorCompaction", majorCompactionCron);
+
+        this.majorcompactionCronTrigger.computeFirstFireTime(null);
+      } catch (Exception e) {
+        LOG.warn(new StringBuilder().append("Failed to parse cron string: ").append(majorCompactionCron).append(". Will use major compaction interval instead. Error").toString(), e);
+      }
+    }
+
+    Pair<List<StoreFile>, List<Path>> loadResults = loadStoreFiles();
+    this.storefiles = sortAndClone((List<StoreFile>)loadResults.getFirst());
 
     // Initialize checksum type from name. The names are CRC32, CRC32C, etc.
     this.checksumType = getChecksumType(conf);
@@ -396,19 +413,34 @@ public class Store extends SchemaConfigured implements HeapSize {
   FileStatus [] getStoreFiles() throws IOException {
     return FSUtils.listStatus(this.fs, this.homedir, null);
   }
+  
+  private Pair<List<StoreFile>, List<Path>> loadStoreFiles() throws IOException {
+		List<Path> paths = new ArrayList<Path>();
+		FileStatus[] files = FSUtils.listStatus(this.fs, this.homedir, null);
+
+		if (files == null) {
+			return loadStoreFiles(null);
+		}
+		for (int i = 0; i < files.length; i++) {
+			if (files[i].isDir()) {
+				continue;
+			}
+			paths.add(files[i].getPath());
+		}
+		return loadStoreFiles(paths);
+	}
 
   /**
    * Creates an unsorted list of StoreFile loaded in parallel
    * from the given directory.
    * @throws IOException
    */
-  private List<StoreFile> loadStoreFiles() throws IOException {
+  private Pair<List<StoreFile>, List<Path>> loadStoreFiles(List<Path> paths) throws IOException {
     ArrayList<StoreFile> results = new ArrayList<StoreFile>();
-    FileStatus files[] = getStoreFiles();
-
-    if (files == null || files.length == 0) {
-      return results;
-    }
+	ArrayList<Path> skipped = new ArrayList<Path>();
+	if ((paths == null) || (paths.isEmpty())) {
+		return new Pair<List<StoreFile>, List<Path>>(results, skipped);
+	}
     // initialize the thread pool for opening store files in parallel..
     ThreadPoolExecutor storeFileOpenerThreadPool =
       this.region.getStoreFileOpenAndCloseThreadPool("StoreFileOpenerThread-" +
@@ -417,67 +449,70 @@ public class Store extends SchemaConfigured implements HeapSize {
       new ExecutorCompletionService<StoreFile>(storeFileOpenerThreadPool);
 
     int totalValidStoreFile = 0;
-    for (int i = 0; i < files.length; i++) {
-      // Skip directories.
-      if (files[i].isDir()) {
-        continue;
-      }
-      final Path p = files[i].getPath();
-      // Check for empty hfile. Should never be the case but can happen
-      // after data loss in hdfs for whatever reason (upgrade, etc.): HBASE-646
-      // NOTE: that the HFileLink is just a name, so it's an empty file.
-      if (!HFileLink.isHFileLink(p) && this.fs.getFileStatus(p).getLen() <= 0) {
-        LOG.warn("Skipping " + p + " because its empty. HBASE-646 DATA LOSS?");
-        continue;
-      }
-
-      // open each store file in parallel
-      completionService.submit(new Callable<StoreFile>() {
-        public StoreFile call() throws IOException {
-          StoreFile storeFile = new StoreFile(fs, p, conf, cacheConf,
-              family.getBloomFilterType(), dataBlockEncoder);
-          passSchemaMetricsTo(storeFile);
-          storeFile.createReader();
-          return storeFile;
-        }
-      });
-      totalValidStoreFile++;
-    }
+	for (final Path p : paths) {
+		// Check for empty hfile. Should never be the case but can happen
+		// after data loss in hdfs for whatever reason (upgrade, etc.):
+		// HBASE-646
+		// NOTE: that the HFileLink is just a name, so it's an empty file.
+		if (this.fs.getFileStatus(p).getLen() <= 0L) {
+			skipped.add(p);
+			LOG.warn(new StringBuilder().append("Skipping ").append(p)
+					.append(" because its empty. HBASE-646 DATA LOSS?")
+					.toString());
+			continue;
+		}
+
+		// open each store file in parallel
+		completionService.submit(new Callable<StoreFile>() {
+			public StoreFile call() throws IOException {
+				StoreFile storeFile = new StoreFile(fs, p, conf, cacheConf,
+						family.getBloomFilterType(), dataBlockEncoder);
+				try {
+					passSchemaMetricsTo(storeFile);
+					storeFile.createReader();
+				} catch (Exception e) {
+					Store.LOG
+							.warn("Failed open of "
+									+ p
+									+ "; presumption is that file was "
+									+ "corrupted at flush and lost edits picked up by commit log replay. "
+									+ "Verify!", e);
+
+					return null;
+				}
+				return storeFile;
+			}
+		});
+		totalValidStoreFile++;
+	}
 
-    IOException ioe = null;
     try {
-      for (int i = 0; i < totalValidStoreFile; i++) {
-        try {
-          Future<StoreFile> future = completionService.take();
-          StoreFile storeFile = future.get();
-          long length = storeFile.getReader().length();
-          this.storeSize += length;
-          this.totalUncompressedBytes +=
-              storeFile.getReader().getTotalUncompressedBytes();
-          if (LOG.isDebugEnabled()) {
-            LOG.debug("loaded " + storeFile.toStringDetailed());
-          }
-          results.add(storeFile);
-        } catch (InterruptedException e) {
-          if (ioe == null) ioe = new InterruptedIOException(e.getMessage());
-        } catch (ExecutionException e) {
-          if (ioe == null) ioe = new IOException(e.getCause());
-        } 
-      } 
-    } finally {
-      storeFileOpenerThreadPool.shutdownNow();
-    }
-    if (ioe != null) {
-      // close StoreFile readers
-      try {
-        for (StoreFile file : results) {
-          if (file != null) file.closeReader(true);
-        }
-      } catch (IOException e) { }
-      throw ioe;
-    }
-
-    return results;
+		for (int i = 0; i < totalValidStoreFile; i++) {
+			Future future = completionService.take();
+			StoreFile storeFile = (StoreFile) future.get();
+			if (storeFile != null) {
+				long length = storeFile.getReader().length();
+				this.storeSize += length;
+				this.totalUncompressedBytes += storeFile.getReader()
+						.getTotalUncompressedBytes();
+
+				if (LOG.isDebugEnabled()) {
+					LOG.debug(new StringBuilder().append("loaded ")
+							.append(storeFile.toStringDetailed())
+							.toString());
+				}
+				results.add(storeFile);
+			}
+		}
+	} catch (InterruptedException e) {
+		throw new IOException(e);
+	} catch (ExecutionException e) {
+		throw new IOException(e.getCause());
+	} finally {
+		storeFileOpenerThreadPool.shutdownNow();
+	}
+
+    return new Pair<List<StoreFile>, List<Path>>(results, skipped);
   }
 
   /**
@@ -1253,7 +1288,26 @@ public class Store extends SchemaConfigured implements HeapSize {
     // TODO: Use better method for determining stamp of last major (HBASE-2990)
     long lowTimestamp = getLowestTimestamp(filesToCompact);
     long now = System.currentTimeMillis();
-    if (lowTimestamp > 0l && lowTimestamp < (now - mcTime)) {
+    
+    boolean timeToMajorCompaction = false;
+
+	if (this.majorcompactionCronTrigger != null) {
+		Date nextFireTime = this.majorcompactionCronTrigger
+				.getNextFireTime();
+		Date nowDate = new Date();
+		if (nowDate.after(nextFireTime)) {
+			timeToMajorCompaction = true;
+			this.majorcompactionCronTrigger.triggered(null);
+			LOG.info(new StringBuilder()
+					.append("Major compaction for store ")
+					.append(toString()).append(" is triggerred by cron.")
+					.toString());
+		}
+	} else if ((lowTimestamp > 0L) && (lowTimestamp < now - mcTime)) {
+		timeToMajorCompaction = true;
+	}
+	
+    if (timeToMajorCompaction) {
       // Major compaction time has elapsed.
       if (filesToCompact.size() == 1) {
         // Single file
-- 
1.8.3.2

